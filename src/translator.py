from typing import Protocol

class IllegalArgumentError(ValueError):
    pass

# * Interfaces for functions
class TokenizeFunction(Protocol):
    def __call__(self, sentence: str) -> list[str]:
        ...

# * Creating basic tokenizers
def tokenize_on_spaces(sentence: str) -> list[str]:
    """Separates out tokens in the sentence based on whitespaces

    Args:
        sentence (str): string to be separated into tokens

    Returns:
        list[str]: list of string tokens
    """
    return sentence.split()

def tokenize_characters(sentence: str) -> list[str]:
    """Separates each characters in the sentence as a separate token

    Args:
        sentence (str): string to be separated into tokens

    Returns:
        list[str]: list of string tokens
    """
    return list(sentence)

_tokenize_functions = {
    "on_spaces": tokenize_on_spaces,
    "characters": tokenize_characters
}

ALLOWED_TOKENIZE_METHODS = list(_tokenize_functions.keys())

# * Creating the Translator

class Translator(Protocol):
    def encode(self, sentence: str) -> list[int]:
        ...

    def __len__(self) -> int:
        ...


class BaseTranslator:
    """Base class for Translators
    """
    def __init__(self, tokenizer: TokenizeFunction, vocab: dict[str, int]):
        """Initialise a Translator

        Args:
            tokenizer: a function that takes as argument a str and returns a list[str]
            vocab: a dictionary with str as keys and int as values. Note that the lowest allowed value is 0!
        """
        self.tokenize = tokenizer
        self.vocab = vocab
        assert min(vocab.values()) >= 0, "Lowest value in vocab cannot be less than 0!"
        assert len(vocab) == max(vocab.values()) + 1, "The largest value in the vocab must be equal to the length of the vocab!"


    def encode(self, sentence: str) -> list[int]:
        return [self.vocab[token] for token in self.tokenize(sentence)]
    
    def __len__(self):
        return len(self.vocab)
    

# * Default Translator

class DefaultTranslator(BaseTranslator):
    """Creates a DefaultTranslator that constructs a vocabulary from a list of sentences, using a chosen tokenizing function.
    """
    def __init__(self, sentences: list[str], tokenizer: TokenizeFunction):
        """
        Args:
            sentences (list[str]): list of sentences in the form of strings
            tokenizer (TokenizeFunction): function to perform tokenization on the sentences.
        """
        self.tokenize = tokenizer
        self.vocab = create_vocab(sentences, tokenizer)

def create_vocab(sentences: list[str], tokenizer: TokenizeFunction) -> dict[str, int]:
    """Creates a vocabulary dictionary which pairs each unique token in the sentences with a unique integer.

    Args:
        sentences (list[str]): list of sentences in the form of strings
        tokenizer (TokenizeFunction): function to perform tokenization on the sentences.

    Returns:
        dict[str, int]: vocabulary dictionary with tokens (str) keys and int values
    """
    sentences = list(set(sentences))  # making sure each sentence is unique, so we don't do redundant operations
    # tokenizing each sentence and then flattening to a set of tokens
    tokenized_sentences = [tokenizer(sentence) for sentence in sentences]
    tokens = {token for sentence in tokenized_sentences for token in sentence}
    # creating a counter
    vocab = {token: i for (i, token) in enumerate(tokens)}
    return vocab

def create_default_translator(sentences: list[str], tokenize_method: str='on_spaces') -> DefaultTranslator:
    """Creates a DefaultTranslator from a list of sentences and a tokenization method

    Args:
        sentences (_type_): list of sentences in the form of strings
        tokenize_method (str): either 'on_spaces' to have tokens generated by splitting sentences on spaces (default),
                               or 'characters' to have tokens be single characters.

    Raises:
        IllegalArgumentError: if any other method than 'on_spaces' or 'characters' is provided.

    Returns:
        DefaultTranslator
    """
    if tokenize_method not in ALLOWED_TOKENIZE_METHODS:
        raise IllegalArgumentError(f"Unknown tokenize method provided. Allowed options are [{', '.join(ALLOWED_TOKENIZE_METHODS)}] but received {tokenize_method}]")
    tokenizer = _tokenize_functions[tokenize_method]
    return DefaultTranslator(sentences, tokenizer)

# TODO (?) implement translator that take synonyms into account, and translator that uses string comparison?