# -*- coding: utf-8 -*-
from setuptools import setup

packages = \
['sentence_similarity']

package_data = \
{'': ['*']}

install_requires = \
['numpy>=1.25.2,<2.0.0',
 'polars>=0.20.31,<0.21.0',
 'pyarrow>=16.1.0,<17.0.0',
 'strsimpy>=0.2.1,<0.3.0']

setup_kwargs = {
    'name': 'sentence-similarity',
    'version': '1.2.1',
    'description': 'Algorithm for fast comparison of string sentences',
    'long_description': '# Sentence Similarity\n\nThis package contains an algorithm which calculates a metric of comparability between a pair of sentences.\nThe package allows for many sentneces to be compared amongst each other quickly (depending on available hardware).\n\n# Installation\n\nThe package can be installed using `pip`:\n\n```\npip install git+https://github.com/LucHeuff/sentence-similarity.git\n```\n\nor using [`poetry`](https://python-poetry.org/):\n\n```\npoetry install git+https://github.com/LucHeuff/sentence-similarity.git\n```\n\nor by adding it to `pyproject.toml`:\n\n```\n[tool.poetry.dependencies]\nsentence-similarity = { git = "https://github.com/LucHeuff/sentence-similarity.git" }\n```\n\n# How does it work?\n\nThe main function of the package is `sentence_similarity(sentences)`, which calculates a similarity score between all\npossible combinations of `sentences` (which needs to be a list of strings).\nThe similarity score ranges from 0 when the two sentences have no tokens in common, to 1 if the sentences exactly match,\nto larger than 1 if one of the sentences is a subset of another sentence, or if tokens are repeated in either of the sentences.\nIf sentences have some but not all tokens in common, or common tokens that are not in the same places in the sentence, the score is between 0 and 1.\n\n**Note** that \'similarity\' should be interpreted as tokens directly matching between two sentences.\nThe algorithm does not take synonyms into account out of the box (though you can add them manually with `create_synonym_vocab`)\nand the algorithm is not smart enough to know when sentences mean the same thing in different languages\n("That car is red" and "Die auto is rood" will compare to a score of 0, since they have no words in common).\n\n# Customization\n\nGiven that there are many ways to tokenize a sentence and to construct a vocabulary, the `sentence_similarity` function\nallows you to provide your own `tokenizer`, `translator` and `vocab` objects to fine tune the similarity algorithm to your own needs.\n\n## Tokenization\n\nA \'token\' is a any set of characters in the form of a string. A `tokenizer` is any function that splits a string into a list of strings.\nThe package contains three tokenizers:\n\n- `tokenize_words`: splits the sentence on spaces, and makes sure that each punctuation mark gets a separate\n  token (For example, `\'What?!\'` is tokenized as `[\'What\', \'?\', \'!\']`).\n- `tokenize_on_spaces`: splits the sentence on spaces, giving punctuation no special treatment (`\'What the?!\'` is tokenized as `[\'What\', \'the?!\']`).\n- `tokenize_characters`: splits each character into a separate token. May be useful when comparing identification codes\n  instead of sentences. Note that different from the other methods, whitespaces receive their own token!\n\nIf these tokenizers do not suit your needs, you can also provide your own:\n\n```\ndef custom_tokenizer(sentence: str) -> list[str]:\n    # your code goes here\n    return tokens\n```\n\nand pass it into the main function using `sentence_similarity(sentences, tokenizer=custom_tokenizer)`.\n\nBy default, `sentence_similarity(sentences)` will use the `tokenize_words` tokenizer.\n\n## Translator\n\nThe Translator is a simple class that performs the translation by first tokenizing the sentence and\nthen passing it through a translation vocabulary. On initialisation, the Translator will also perform\nsome checks on the vocabulary that are intended to optimise performance.\nFor instance, the smallest value in the vocab **must be 0**, and the largest value of the vocab **must be equal to its length**.\nThe sentence similarity algorithm uses matrices to calculate the similarity score, and these requirements makes\nsure that these matrices do not get any larger than they need to be.\n\n`sentence_similarity(sentences)` will create a default Translator for you if you do not provide one.\nIt will split the sentences using the provided tokenizer (`tokenize_words` if you don\'t provide any), and\nwill create a vocabulary from all those tokens.\n\nIf you want to use a different vocabulary, you can use the `create_translator(tokenizer, vocab)` convenience function\nto create a custom Translator and pass it into the main function using `sentence_similarity(sentences, translator=custom_translator)`.\n\nDepending on the vocabulary generation method, the creation of the Translator can be very timeconsuming. In this case it\nmay be worthwhile to create the Translator separately, then pass it into the `sentence_similarity` function whenever it is called.\nThis can be especially helpful if the `sentence_similarity` function is called many times, using the same Translator.\n\n## Vocabularies\n\nA vocabulary is simply a dictionary which translates strings into an integers: `dict[str, int]`. Any such dictionary\ncan be used, so you can provide your own, as long as the **smallest integer is 0** and **the largest integer is equal to the lenght of the dictionary** (see above),\n\nThe package contains three methods of creating vocabularies:\n\n- `create_vocab(sentences, tokenizer)`:  \n  splits all sentences into tokens based on a provided `tokenizer` and gives each token a unique integer value.\n- `create_synonym_vocab(sentences, synonyms, tokenizer)`:  \n  allows you to additionally pass in a list of tuples, where each tuple contains all the words that are synonyms of each other\n  (e.g. `[(\'motor\', \'engine\'), (\'car\', \'van\', \'SUV\'), ...]`)\n  Each token in a set of synonyms is translated to the same integer value.\n- `create_string_distance_vocab(sentences, distance, tokenizer, distance_function)`:  \n  allows for tokens that are within `distance` from each other based on some `distance_function` to be translated to the same integer value.\n  The `distance_function` is assumed to be a `StringDistance` function from the [`strsimpy`](https://github.com/luozhouyang/python-string-similarity) package (defaults to `Levenshtein`).\n  This can be useful when there may be typo\'s in your sentences.\n  **Note** that string distances can take a while to calculate!\n\n## Weight matrix\n\nThe algorithm uses a \'weight matrix\' in order to discount tokens that match but are not in the same position in the sentence.\nBy default, the weight matrix ranges from 1 on the diagonal to 0.1 on the bottom-left and top-right corners, decreasing linearly\nalong the way. The minimum value at the edges can be customised by setting the `weight_matrix_min` value to a float between 0 and 1, for example:\n`sentence_similarity(sentences, weight_matrix_min=0.5)`. Setting this value will change how the score responds to tokens that match\nbetween sentences but are in different places. Setting the value to 1. disables discounting entirely.    \nIf you alternatively want to ignore any tokens that are not in exactly the same position between the two sentences, you can use `sentence_similarity(sentences, weight_matrix_min=\'identity\')` instead,\nwhich will set the weight matrix to an identity matrix.\n',
    'author': 'Luc Heuff',
    'author_email': 'lucheuff@hotmail.com',
    'maintainer': 'None',
    'maintainer_email': 'None',
    'url': 'None',
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.11,<4.0',
}


setup(**setup_kwargs)

